# üìò Tensor, Perceptron, Single-Layer & Multi-Layer Perceptron (MLP)

A **complete, beginner-to-advanced, technical yet easy-to-understand guide**, written as a **professional GitHub-ready README.md**.

---

## üìë Table of Contents

1. What is a Tensor?
2. Why Tensors Matter in Machine Learning & Deep Learning
3. Mathematical Representation of Tensors
4. Tensor vs Vector vs Matrix
5. Tensor Operations (with Examples)
6. What is a Perceptron?
7. Mathematical Model of a Perceptron
8. Activation Functions (Detailed)
9. Limitations of a Single Perceptron
10. Single-Layer Perceptron (SLP)
11. Multi-Layer Perceptron (MLP)
12. Forward Propagation in MLP
13. Loss Function
14. Backpropagation (Concept + Math)
15. Training an MLP Step-by-Step
16. Code Examples (NumPy & Scikit-Learn)
17. SLP vs MLP Comparison Table
18. Key Takeaways

---

## 1Ô∏è‚É£ What is a Tensor?

A **tensor** is a **generalized mathematical structure** used to represent data in machine learning and deep learning.

> In simple terms:

* **Scalar** ‚Üí single number
* **Vector** ‚Üí list of numbers
* **Matrix** ‚Üí table of numbers
* **Tensor** ‚Üí data with **any number of dimensions**

### Examples

| Data              | Representation                     | Tensor Rank |
| ----------------- | ---------------------------------- | ----------- |
| Temperature       | 25                                 | 0 (Scalar)  |
| House prices      | [100, 200, 300]                    | 1 (Vector)  |
| Image (grayscale) | 28√ó28                              | 2 (Matrix)  |
| Image (RGB)       | 28√ó28√ó3                            | 3 (Tensor)  |
| Video             | Frames √ó Height √ó Width √ó Channels | 4 (Tensor)  |

---

## 2Ô∏è‚É£ Why Tensors Matter

Deep learning models **do not work on raw data**.
They operate on **tensors**.

Examples:

* Text ‚Üí token embeddings ‚Üí tensor
* Image ‚Üí pixel values ‚Üí tensor
* Audio ‚Üí spectrogram ‚Üí tensor

Frameworks like **TensorFlow** and **PyTorch** are built entirely around tensors.

---

## 3Ô∏è‚É£ Mathematical Representation of a Tensor

A tensor is represented as:

* Rank-0: `T`
* Rank-1: `T[i]`
* Rank-2: `T[i][j]`
* Rank-3: `T[i][j][k]`

Example:

```
T[batch_size][height][width][channels]
```

---

## 4Ô∏è‚É£ Tensor vs Vector vs Matrix

| Concept | Dimensions | Example                      |
| ------- | ---------- | ---------------------------- |
| Scalar  | 0D         | 5                            |
| Vector  | 1D         | [1, 2, 3]                    |
| Matrix  | 2D         | [[1,2],[3,4]]                |
| Tensor  | nD         | Image, Video, NLP Embeddings |

---

## 5Ô∏è‚É£ Tensor Operations

### Common Operations

* Addition
* Multiplication
* Dot product
* Matrix multiplication
* Broadcasting

### Example (NumPy)

```python
import numpy as np

A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

C = np.dot(A, B)
print(C)
```

---

## 6Ô∏è‚É£ What is a Perceptron?

A **perceptron** is the **simplest neural network unit**.

> It mimics a biological neuron.

### Components

* Inputs (features)
* Weights
* Bias
* Activation function

---

## 7Ô∏è‚É£ Mathematical Model of a Perceptron

### Formula

```
Z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b
```

```
Output = Activation(Z)
```

### Step-by-Step

1. Multiply inputs with weights
2. Add bias
3. Apply activation

---

## 8Ô∏è‚É£ Activation Functions

### Why Activation?

Without activation, a neural network becomes **just linear regression**.

### Common Activations

| Function | Formula   | Use Case              |
| -------- | --------- | --------------------- |
| Step     | 0 or 1    | Classic Perceptron    |
| Sigmoid  | 1/(1+e‚ÅªÀ£) | Binary classification |
| ReLU     | max(0, x) | Deep networks         |
| Tanh     | (-1,1)    | Centered data         |

---

## 9Ô∏è‚É£ Limitations of a Single Perceptron

A single perceptron:

* Can only solve **linearly separable problems**
* Cannot solve **XOR problem**

This limitation led to **multi-layer networks**.

---

## üîü Single-Layer Perceptron (SLP)

### Definition

An SLP consists of:

* Input layer
* One output layer
* No hidden layer

### Architecture

```
Input ‚Üí Weights ‚Üí Output
```

### Use Cases

* Spam detection (simple)
* Binary classification

---

## 1Ô∏è‚É£1Ô∏è‚É£ Multi-Layer Perceptron (MLP)

### Definition

An **MLP** is a neural network with:

* Input layer
* One or more **hidden layers**
* Output layer

### Architecture

```
Input ‚Üí Hidden Layer(s) ‚Üí Output
```

### Why Hidden Layers Matter

They allow the network to learn:

* Non-linear relationships
* Complex decision boundaries

---

## 1Ô∏è‚É£2Ô∏è‚É£ Forward Propagation

### Steps

1. Input tensor enters the network
2. Each layer computes weighted sum
3. Activation applied
4. Output generated

### Mathematical Form

```
Z‚ÇÅ = XW‚ÇÅ + b‚ÇÅ
A‚ÇÅ = ReLU(Z‚ÇÅ)
Z‚ÇÇ = A‚ÇÅW‚ÇÇ + b‚ÇÇ
Output = Sigmoid(Z‚ÇÇ)
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ Loss Function

Measures **how wrong the prediction is**.

### Examples

| Loss                 | Use Case              |
| -------------------- | --------------------- |
| MSE                  | Regression            |
| Binary Cross-Entropy | Binary classification |
| Categorical CE       | Multi-class           |

---

## 1Ô∏è‚É£4Ô∏è‚É£ Backpropagation

### What is Backpropagation?

A method to **update weights** using gradients.

### Steps

1. Compute loss
2. Calculate gradient of loss w.r.t weights
3. Update weights using gradient descent

```
w = w - learning_rate √ó gradient
```

---

## 1Ô∏è‚É£5Ô∏è‚É£ Training an MLP (Step-by-Step)

1. Initialize weights randomly
2. Forward propagation
3. Compute loss
4. Backpropagation
5. Update weights
6. Repeat for epochs

---

## 1Ô∏è‚É£6Ô∏è‚É£ Code Examples

### Single-Layer Perceptron (NumPy)

```python
import numpy as np

X = np.array([[0,0],[0,1],[1,0],[1,1]])
y = np.array([0,0,0,1])

w = np.random.rand(2)
b = 0.0
lr = 0.1

for epoch in range(100):
    for i in range(len(X)):
        z = np.dot(X[i], w) + b
        y_pred = 1 if z >= 0 else 0
        error = y[i] - y_pred
        w += lr * error * X[i]
        b += lr * error

print(w, b)
```

### MLP using Scikit-Learn

```python
from sklearn.neural_network import MLPClassifier

X = [[0,0],[0,1],[1,0],[1,1]]
y = [0,1,1,0]

model = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', max_iter=1000)
model.fit(X, y)

print(model.predict(X))
```

---

## 1Ô∏è‚É£7Ô∏è‚É£ SLP vs MLP Comparison

| Feature       | SLP          | MLP              |
| ------------- | ------------ | ---------------- |
| Hidden Layers | ‚ùå            | ‚úÖ                |
| Non-linearity | ‚ùå            | ‚úÖ                |
| XOR Problem   | ‚ùå            | ‚úÖ                |
| Complexity    | Low          | High             |
| Use Case      | Simple tasks | Complex problems |

---

## 1Ô∏è‚É£8Ô∏è‚É£ Key Takeaways

* **Tensors** are the backbone of deep learning
* **Perceptron** is the basic building block
* **SLP** works only for linear problems
* **MLP** solves complex, real-world tasks
* Hidden layers + activation = power of neural networks

---

## üìå Final Note

This README is designed to be:

* Beginner friendly
* Technically accurate
* Interview ready
* Production ready

You can directly **download, fork, or extend** this for your GitHub projects.

---

‚≠ê If you want: CNN, RNN, Backprop math derivation, or interview Q&A ‚Äî just tell me.
