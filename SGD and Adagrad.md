# ğŸš€ Gradient Descent Optimizers: SGD, SGD with Momentum, AdaGrad

A **professional, GitHubâ€‘ready README** explaining **Stochastic Gradient Descent (SGD)**, **SGD with Momentum**, and **AdaGrad** â€” from absolute basics to implementation â€” in **simple, clear language** with **mathematics intuition, diagrams explanation, real examples, and Python code**.

---

## ğŸ“Œ Table of Contents

1. Introduction to Optimization in Machine Learning
2. What is Gradient Descent?
3. Batch vs Stochastic vs Miniâ€‘Batch Gradient Descent
4. Stochastic Gradient Descent (SGD)

   * Intuition
   * Mathematical Formulation
   * Advantages & Disadvantages
   * Python Implementation (From Scratch)
   * SGD in Scikitâ€‘Learn
5. SGD with Momentum

   * Why Momentum is Needed
   * Physical Intuition
   * Mathematical Formulation
   * Python Implementation (From Scratch)
   * SGD with Momentum in Deep Learning (PyTorch)
6. AdaGrad (Adaptive Gradient Algorithm)

   * Why Adaptive Learning Rates?
   * Mathematical Intuition
   * Python Implementation (From Scratch)
   * AdaGrad in Scikitâ€‘Learn / PyTorch
7. Comparison Table
8. When to Use Which Optimizer?
9. Key Takeaways

---

## 1ï¸âƒ£ Introduction to Optimization in Machine Learning

In machine learning, **training a model** means **finding the best parameters (weights)** that minimize a **loss function**.

This is done using **optimization algorithms**, also called **optimizers**.

ğŸ‘‰ Popular optimizers include:

* Gradient Descent
* SGD
* Momentum
* AdaGrad
* RMSProp
* Adam

This README focuses on:

* **SGD**
* **SGD with Momentum**
* **AdaGrad**

---

## 2ï¸âƒ£ What is Gradient Descent?

Gradient Descent is an **iterative optimization algorithm** used to minimize a function.

### ğŸ”¹ Core Idea

* Compute the **gradient (slope)** of the loss function
* Move **opposite to the gradient**
* Repeat until minimum loss is reached

### ğŸ”¹ Update Rule

```
w = w - learning_rate Ã— gradient
```

Where:

* `w` â†’ model parameters
* `learning_rate (Î·)` â†’ step size
* `gradient` â†’ derivative of loss w.r.t. parameters

---

## 3ï¸âƒ£ Batch vs Stochastic vs Miniâ€‘Batch Gradient Descent

| Type          | Data Used      | Speed    | Stability   |
| ------------- | -------------- | -------- | ----------- |
| Batch GD      | Entire dataset | Slow     | Very Stable |
| **SGD**       | Single sample  | Fast     | Noisy       |
| Miniâ€‘Batch GD | Small batch    | Balanced | Stable      |

ğŸ‘‰ **SGD** is widely used for **large datasets** and **deep learning**.

---

## 4ï¸âƒ£ Stochastic Gradient Descent (SGD)

### ğŸ”¹ What is SGD?

**Stochastic Gradient Descent** updates model parameters using **one data point at a time**.

Instead of computing gradient over full dataset:

```
For each data point:
    compute gradient
    update weights
```

---

### ğŸ”¹ Intuition

* Faster updates
* Noisy path toward minimum
* Can escape local minima

Think of SGD like **walking downhill with random small steps**.

---

### ğŸ”¹ Mathematical Formulation

For a single training example `(xáµ¢, yáµ¢)`:

```
w = w - Î· Ã— âˆ‡L(w; xáµ¢, yáµ¢)
```

Where:

* `L` â†’ loss function
* `Î·` â†’ learning rate

---

### ğŸ”¹ Advantages

* Faster convergence for large datasets
* Requires less memory
* Can escape saddle points

### ğŸ”¹ Disadvantages

* Noisy updates
* Can oscillate near minimum
* Sensitive to learning rate

---

### ğŸ”¹ Python Implementation (From Scratch)

```python
import numpy as np

# Sample dataset
X = np.array([[1], [2], [3], [4]])
y = np.array([2, 4, 6, 8])

# Initialize weight and bias
w = 0.0
b = 0.0
lr = 0.01

# SGD training
for epoch in range(100):
    for i in range(len(X)):
        y_pred = w * X[i] + b
        error = y_pred - y[i]

        # gradients
        dw = error * X[i]
        db = error

        # update
        w -= lr * dw
        b -= lr * db

print("Weight:", w)
print("Bias:", b)
```

---

### ğŸ”¹ SGD in Scikitâ€‘Learn

```python
from sklearn.linear_model import SGDRegressor

model = SGDRegressor(max_iter=1000, learning_rate='constant', eta0=0.01)
model.fit(X, y)

print(model.coef_, model.intercept_)
```

---

## 5ï¸âƒ£ SGD with Momentum

### ğŸ”¹ Why Momentum?

SGD suffers from:

* Zigâ€‘zag movement
* Slow convergence in narrow valleys

**Momentum** helps accelerate SGD in the correct direction.

---

### ğŸ”¹ Physical Intuition

Imagine a **ball rolling downhill**:

* Gains speed
* Resists sudden direction changes

Momentum remembers **past gradients**.

---

### ğŸ”¹ Mathematical Formulation

```
v = Î³v + Î·âˆ‡L(w)
w = w - v
```

Where:

* `v` â†’ velocity
* `Î³` â†’ momentum coefficient (usually 0.9)

---

### ğŸ”¹ Python Implementation (From Scratch)

```python
w = 0.0
b = 0.0
v_w = 0.0
v_b = 0.0
lr = 0.01
momentum = 0.9

for epoch in range(100):
    for i in range(len(X)):
        y_pred = w * X[i] + b
        error = y_pred - y[i]

        dw = error * X[i]
        db = error

        v_w = momentum * v_w + lr * dw
        v_b = momentum * v_b + lr * db

        w -= v_w
        b -= v_b

print(w, b)
```

---

### ğŸ”¹ Momentum in PyTorch

```python
import torch

optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

---

## 6ï¸âƒ£ AdaGrad (Adaptive Gradient Algorithm)

### ğŸ”¹ Why AdaGrad?

Different parameters may need **different learning rates**.

AdaGrad adapts learning rate **per parameter**.

---

### ğŸ”¹ Mathematical Intuition

Parameters with **large gradients** â†’ smaller learning rate
Parameters with **small gradients** â†’ larger learning rate

---

### ğŸ”¹ Update Rule

```
G = G + (âˆ‡L)^2
w = w - (Î· / âˆš(G + Îµ)) Ã— âˆ‡L
```

Where:

* `G` â†’ accumulated squared gradients
* `Îµ` â†’ small constant for stability

---

### ğŸ”¹ Python Implementation (From Scratch)

```python
w = 0.0
b = 0.0
G_w = 0.0
G_b = 0.0
lr = 0.1
epsilon = 1e-8

for epoch in range(100):
    for i in range(len(X)):
        y_pred = w * X[i] + b
        error = y_pred - y[i]

        dw = error * X[i]
        db = error

        G_w += dw ** 2
        G_b += db ** 2

        w -= (lr / (np.sqrt(G_w) + epsilon)) * dw
        b -= (lr / (np.sqrt(G_b) + epsilon)) * db

print(w, b)
```

---

### ğŸ”¹ AdaGrad in PyTorch

```python
optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)
```

---

## 7ï¸âƒ£ Comparison Table

| Optimizer      | Learning Rate  | Speed    | Stability | Best For        |
| -------------- | -------------- | -------- | --------- | --------------- |
| SGD            | Fixed          | Fast     | Low       | Large datasets  |
| SGD + Momentum | Fixed + memory | Faster   | Medium    | Deep networks   |
| AdaGrad        | Adaptive       | Moderate | High      | Sparse features |

---

## 8ï¸âƒ£ When to Use Which Optimizer?

* **SGD** â†’ simple problems, large datasets
* **Momentum** â†’ deep learning, faster convergence
* **AdaGrad** â†’ NLP, sparse data, word embeddings

---

## 9ï¸âƒ£ Key Takeaways

* SGD updates weights per data point
* Momentum accelerates learning using velocity
* AdaGrad adapts learning rate automatically
* Choice of optimizer greatly affects convergence

---

## â­ Final Note

This README is **downloadâ€‘ready**, **GitHubâ€‘friendly**, and **productionâ€‘oriented**.

If you want:

* Adam & RMSProp
* Visual loss plots
* Interview questions
* Mathematical proofs

Just tell me â€” happy to extend ğŸš€
